import math
import tensorflow as tf
import wmodule
import functools
from .backbone import Backbone
from .build import BACKBONE_REGISTRY,build_backbone_by_name
from .resnet import build_resnet_backbone
from .resnetv2 import build_resnetv2_backbone
from .shufflenetv2 import build_shufflenetv2_backbone
import collections
import object_detection2.od_toolkit as odt
from .build import build_backbone_hook
import wnnlayer as wnnl
from object_detection2.datadef import *
import basic_tftools as btf

slim = tf.contrib.slim

class DLA(Backbone):

    def __init__(
        self, cfg,bottom_up, in_features,
            parent=None,*args,**kwargs
    ):
        """
        Args:
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in `in_features`,
                are used to generate DLA levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which DLA is attached. For example, if the
                backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
                of these may be used; order must be from high to low resolution.
            norm (str): the normalization to use.
        """
        stage = int(in_features[-1][-1:])
        super(DLA, self).__init__(cfg,parent=parent,*args,**kwargs)
        assert isinstance(bottom_up, Backbone)
        def get_feature_name(x):
            p = x.find(":")
            if p<0:
                return x
            else:
                return x[:p]
        # Place convs into top-down order (from low to high resolution)
        # to make the top-down computation in forward clearer.
        self.in_features = [get_feature_name(x) for x in in_features]
        self.bottom_up = bottom_up
        self.scope = "DLA"
        self.interpolate_op=tf.image.resize_nearest_neighbor
        self.stage = stage
        #Detectron2默认没有使用normalizer, 但在测试数据集上发现不使用normalizer网络不收敛
        self.normalizer_fn,self.norm_params = odt.get_norm(self.cfg.MODEL.DLA.NORM,self.is_training)
        self.hook_before,self.hook_after = build_backbone_hook(cfg.MODEL.DLA,parent=self)
        self.activation_fn = odt.get_activation_fn(self.cfg.MODEL.DLA.ACTIVATION_FN)
        self.out_channels = [ 64,64,128,256]
        self.conv_op = functools.partial(slim.conv2d,normalizer_fn=self.normalizer_fn,
                                         normalizer_params=self.norm_params)
        self.upsample_op = functools.partial(slim.conv2d_transpose,
                                             kernel_size=4,
                                             stride=2,
                                             normalizer_fn=self.normalizer_fn,
                                             normalizer_params=self.norm_params)


    @property
    def size_divisibility(self):
        return self._size_divisibility

    def forward(self, x):
        """
        Args:
            input (dict[str->Tensor]): mapping feature map name (e.g., "res5") to
                feature map tensor for each feature level in high to low resolution order.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to DLA feature map tensor
                in high to low resolution order. Returned feature names follow the DLA
                paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
                ["p2", "p3", ..., "p6"].
        """
        bottom_up_features = self.bottom_up(x)
        if self.hook_before is not None:
            bottom_up_features = self.hook_before(bottom_up_features,x)
        image_features = [bottom_up_features[f] for f in self.in_features]

        with tf.variable_scope(self.scope, 'DLA'):
            features = self.dla_up(image_features)
            self._dla_upv2(features,1,len(features),scope="ida_up")
        res = OrderedDict()
        res.update(bottom_up_features)
        k = self.in_features[-1]
        index = k[-1:]
        res[f"P{index}"] = tf.identity(features[-1],name="output")
        if self.hook_after is not None:
            res = self.hook_after(res, x)
        return res

    def dla_up(self,features):
        '''
        features: 分辨率从高到低
        '''
        features = list(features)
        outs = [features[-1]]
        with tf.variable_scope("dla_up"):
            x = list(reversed(range(1,len(features))))
            print(x)
            for i in reversed(range(1,len(features))):
                self._dla_upv1(features,i,len(features),
                             scope=f"sub_dla_up_{i}")
                outs.insert(0,features[-1])
        return outs


    def _dla_upv1(self,features,startp,endp,scope="ida"):
        conv_op = self.conv_op
        upsample_op = self.upsample_op
        with tf.variable_scope(scope):
            for i in range(startp,endp):
                C = btf.channel(features[i-1])
                x = conv_op(features[i],C,[3,3],scope=f"project_{i}")
                x = upsample_op(x,C,scope=f"upsample_{i}")
                features[i] = conv_op(x+features[i-1],C,[3,3],scope=f"node_{i}")

    def _dla_upv2(self,features,startp,endp,scope="ida"):
        conv_op = self.conv_op
        with tf.variable_scope(scope):
            for i in range(startp,endp):
                upsample_op = functools.partial(slim.conv2d_transpose,
                                                kernel_size=4**i,
                                                stride=2**i,
                                                normalizer_fn=self.normalizer_fn,
                                                normalizer_params=self.norm_params)
                C = btf.channel(features[i-1])
                x = conv_op(features[i],C,[3,3],scope=f"project_{i}")
                x = upsample_op(x,C,scope=f"upsample_{i}")
                features[i] = conv_op(x+features[i-1],C,[3,3],scope=f"node_{i}")

@BACKBONE_REGISTRY.register()
def build_any_dla_backbone(cfg,*args,**kwargs):
    """
    Args:
        cfg: a CfgNode

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_backbone_by_name(cfg.MODEL.DLA.BACKBONE,cfg, *args,**kwargs)
    in_features = cfg.MODEL.DLA.IN_FEATURES
    backbone = DLA(
        bottom_up=bottom_up,
        in_features=in_features,
        cfg=cfg,
        *args,
        **kwargs
    )
    return backbone
